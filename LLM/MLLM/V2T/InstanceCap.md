# InstanceCap: Improving Text-to-Video Generation via Instance-aware Structured Caption

## 研究背景

现有的 Caption 存在以下问题：</br>

+ 细节不足：难以全面捕捉视频中每个实例（object）的具体信息。
+ 幻觉现象：生成的描述中容易出现不真实或多余的信息。
+ 运动描述不准确：对视频中物体的动作和运动捕捉不精确。

## 主要贡献

1. 实例感知结构化字幕框架（InstanceCap）：首次实现了实例级、细粒度的视频字幕生成方法，使得每个视频中的关键对象能够被单独描述。
  
2. 辅助模型集群（Auxiliary Models Cluster, AMC）：利用多个辅助模型（如目标检测、实例分割和摄像机运动预测）对视频进行预处理，将全局视频分解为局部实例，提升了字幕与视频间的信息保真度。
   这一模块能够将视频中的关键且细粒度的实例（如人物、车辆等）提取出来。 
3. 改进的链式思考（Chain-of-Thought, CoT）提示流程：通过预设的细粒度 prompt（例如要求描述实例的类别、外观、动作、位置等），利用多模态大语言模型（MLLMs）将密集提示转化为结构化短语，生成简洁且准确的描述。
4. InstanceVid 数据集：策划了一个包含约2.2万个视频实例的数据集，为训练提供了高质量的实例级字幕数据。
5. InstanceEnhancer 推断流程：针对训练时与推断时字幕分布不一致的问题，提出了一个增强流程，使得输入的短提示能够转换为符合结构化字幕格式的描述。
   
## 评估指标

+ 3DVAE Score
+ CLIP Score
+ 人工评估

## FAQ

+ 为什么 InstanceCap 在无主体视频样本上生成的 caption 的结果很差：
  1. 辅助模型集群的依赖性：InstanceCap 的核心在于利用辅助模型集群（Auxiliary Models Cluster, AMC）进行目标检测、实例分割以及摄像机运动预测，从而将全局视频转换为局部的实例信息。论文中提到，这一模块能够将视频中的关键实例（如人物、车辆等）提取出来，为后续的细粒度描述提供信息基础。
    + 对于无主体的视频，目标检测和实例分割模块可能无法识别出明显的实例或只能检测到一些低信噪比的背景噪音。
    + 当输入给后续的多模态大语言模型（MLLM）时，这些“虚假”或缺乏语义意义的检测结果无法提供有价值的提示，从而导致生成的字幕缺乏准确性和细节，甚至产生幻觉或冗余描述。
  2. 链式思考（Chain-of-Thought, CoT）提示策略的局限：InstanceCap 采用了改进的 CoT prompt 流程，通过预设的细粒度 prompt（例如要求描述实例的类别、外观、动作、位置等）来引导 MLLM 生成结构化字幕。这种提示策略在视频中存在明显实例时，可以有效帮助模型生成精细描述。
    + 在无主体的视频场景下，由于缺少明确的实例信息，这些 prompt 往往得不到足够的信息支撑。
    + 模型在面对“空洞”或低信息密度的提示时，容易“猜测”或生成一些默认的、泛泛而谈的描述，从而导致字幕效果不佳。
  3. OOD 问题：InstanceCap 的训练依赖于作者构建的 22K 条实例视频数据集（InstanceVid），该数据集主要来源于具有明显实例特征的视频场景。数据集中每个视频都伴随了经过精心标注的实例级结构化字幕，使得模型在训练时“习惯”于从清晰的实例信息中学习描述生成。
    + 输入数据的分布与训练数据存在较大差异，即存在分布外（out-of-distribution, OOD）问题。这种分布的不匹配导致模型无法很好地泛化到无主体场景，生成的字幕往往缺乏针对性和详细信息。